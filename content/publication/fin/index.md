---
abstract: 'To advance Chinese financial natural language processing (NLP), we introduce BBT-FinT5, a new Chinese financial pre-training language model based on the T5 model. To support this effort, we have built BBT-FinCorpus, a large-scale financial corpus with approximately 300GB of raw text from four different sources. In general domain NLP, comprehensive benchmarks like GLUE and SuperGLUE have driven significant advancements in language model pre-training by enabling head-to-head comparisons among models. Drawing inspiration from these benchmarks, we propose BBT-CFLEB, a Chinese Financial Language understanding and generation Evaluation Benchmark, which includes six datasets covering both understanding and generation tasks. Our aim is to facilitate research in the development of NLP within the Chinese financial domain. Our model, corpus and benchmark are released at this https URL. Our work belongs to the Big Bang Transformer (BBT), a large-scale pre-trained language model project.'

slides: ""
url_pdf: 'https://arxiv.org/abs/2302.09432'
publication_types:
    - 'Preprint'
authors:
    - 'Dakuan Lu' 
    - 'Hengkui Wu' 
    - 'Jiaqing Liang' 
    - 'Yipei Xu' 
    - admin
    - 'Yipeng Geng' 
    - 'Mengkun Han' 
    - 'Yingsi Xin' 
    - 'Yanghua Xiao'
author_notes: []
publication: Preprint
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: 'BBT-Fin: Comprehensive Construction of Chinese Financial Domain Pre-trained Language Model, Corpus and Benchmark'
doi: ""
featured: false
tags: []
projects: []
image:
    caption: ""
    focal_point: ""
    preview_only: false
    filename: fin.png
date: '2023-02-10T02:34:00.000Z'
url_slides: ""
publishDate: '2023-02-13T00:00:00.000Z'
url_poster: ""
url_code: 'https://github.com/ssymmetry/BBT-FinCUGE-Applications'
---
