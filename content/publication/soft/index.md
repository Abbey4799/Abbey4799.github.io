---
abstract: 'It is crucial for large language models (LLMs)
to follow instructions that involve multiple constraints. However, it is an unexplored area
to enhance LLMs’ ability to follow soft constraints. To bridge the gap, we initially design a
pipeline to construct datasets with high-quality
outputs automatically. Additionally, to fully
utilize the positive and negative samples generated during the data construction process, we
choose Direct Preference Optimization (DPO)
as the training method. Furthermore, taking
into account the difficulty of soft constraints
indicated by the number of constraints, we design a curriculum learning training paradigm
based on the constraint quantity. We experimentally evaluate the effectiveness of our methods in improving LLMs’ soft constraint following ability and analyze the factors driving
the improvements. '
slides: ""
url_pdf: 'https://arxiv.org/abs/2501.04945'
publication_types:
    - 'Conference Paper'
authors:
    - 'Qingyu Ren'
    - 'Jie Zeng'
    - admin
    - 'Jiaqing Liang'
    - 'Yanghua Xiao'
    - 'Weikang Zhou'
    - 'Zeye Sun'
    - 'Fei Yu'
author_notes: []
publication: In ***ACL 2025 Findings***
summary: ""
url_dataset: ""
url_project: ""
publication_short: ""
url_source: ""
url_video: ""
title: 'Step-by-Step Mastery: Enhancing Soft Constraint Following Ability of Large Language Models'
doi: ""
featured: true
tags: []
projects: []
image:
    caption: ""
    focal_point: ""
    preview_only: false
    filename: soft.png
date: '2025-03-28T02:34:00.000Z'
url_slides: ""
publishDate: '2025-03-28T00:00:00.000Z'
url_poster: ""
url_code: 'https://github.com/Rainier-rq/FollowSoftConstraint'
---
